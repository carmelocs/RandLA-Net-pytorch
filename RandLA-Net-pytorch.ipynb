{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "c360f7f21a72e5083ad62d0bcf8bfd45b1ab32f2b7b7db5d7b0dd8bb45545b1c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reading note for \"RandLA-Net\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_points_kernels import knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rand_idx: [81 47 54 54 91 49 95  3 23 73]\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.choice(100, 10)\n",
    "print(f\"rand_idx: {rand_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pc: torch.Size([8, 16, 7])\npc_xyz: torch.Size([8, 16, 3])\nidx: torch.Size([8, 16, 4])\nextended_idx: torch.Size([8, 3, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "# fake data\n",
    "torch.manual_seed(0)\n",
    "\n",
    "BATCH = 8\n",
    "NUM_POINT = 2**4\n",
    "D_IN = 3+4\n",
    "NUM_NEIGHBOUR = 4\n",
    "\n",
    "pc = torch.rand(BATCH, NUM_POINT, D_IN)\n",
    "print(f\"pc: {pc.shape}\")\n",
    "pc_xyz = pc[:, :, :3]\n",
    "print(f\"pc_xyz: {pc_xyz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "idx: torch.Size([8, 16, 4])\nextended_idx: torch.Size([8, 3, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "idx, dist = knn(pc_xyz.cpu().contiguous(), pc_xyz.cpu().contiguous(), NUM_NEIGHBOUR)\n",
    "print(f\"idx: {idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extended_idx: torch.Size([8, 3, 16, 4])\nextended_dist: torch.Size([8, 3, 16, 4])\nextended_xyz: torch.Size([8, 3, 16, 4])\nneighbour: torch.Size([8, 3, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "extended_idx = idx.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "print(f\"extended_idx: {extended_idx.shape}\")\n",
    "extended_dist = dist.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "print(f\"extended_dist: {extended_dist.shape}\")\n",
    "extended_xyz = pc_xyz.transpose(-2, -1).unsqueeze(-1).repeat(1, 1, 1, NUM_NEIGHBOUR)\n",
    "print(f\"extended_xyz: {extended_xyz.shape}\")\n",
    "neighbour = extended_xyz.gather(dim=2, index=extended_idx)\n",
    "print(f\"neighbour: {neighbour.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, bn=False, activation_fn=None):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if bn else None\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass of the network\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            input: torch.Tensor, shape (B, d_in, N, K)\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor, shape (B, d_out, N, K)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.activation_fn is not None:\n",
    "            x = self.activation_fn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalSpatialEncoding(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(LocalSpatialEncoding, self).__init__()\n",
    "\n",
    "        self.mlp = MLP(in_channels=3+3+3+1, out_channels=out_channels, bn=True, activation_fn=nn.ReLU())\n",
    "\n",
    "    def forward(self, point, knn_output):\n",
    "        '''\n",
    "        Input:\n",
    "            point: [B, N, 3+d]\n",
    "            knn_output: tuple\n",
    "        Output:\n",
    "            neighbouring_feat: [B, 2*d, N, K]\n",
    "\n",
    "        '''\n",
    "\n",
    "        idx, dist = knn_output  # [B, N, K]\n",
    "        B, N, K = idx.size()\n",
    "\n",
    "        xyz = point[:, :, :3]  # [B, N, 3]\n",
    "        feat = point[:, :, 3:]  # [B, N, d]\n",
    "\n",
    "        extended_idx = idx.unsqueeze(1).repeat(1, 3, 1, 1)  # [B, 3, N, K]\n",
    "        extended_xyz = xyz.transpose(-2, -1).unsqueeze(-1).repeat(1, 1, 1, K)  # [B, 3, N, K]\n",
    "        neighbour = extended_xyz.gather(dim=2, index=extended_idx)  # [B, 3, N, K]\n",
    "        concat_xyz = torch.cat((extended_xyz, neighbour, extended_xyz - neighbour, dist.unsqueeze(1)), dim=1) # [B, 10, N, K]\n",
    "        relative_pnt_pos_enc = self.mlp(concat_xyz)  # [B, out_channels, N, K]\n",
    "        output = torch.cat((relative_pnt_pos_enc, feat.transpose(-2, -1).unsqueeze(-1).repeat(1, 1, 1, K)), dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LoSE features: torch.Size([8, 8, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "LoSE = LocalSpatialEncoding(out_channels=4)\n",
    "knn_output = knn(pc_xyz.cpu().contiguous(), pc_xyz.cpu().contiguous(), NUM_NEIGHBOUR)\n",
    "lose_feat = LoSE(pc, knn_output)\n",
    "print(f\"LoSE features: {lose_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentivePooling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentivePooling, self).__init__()\n",
    "\n",
    "        self.score_fn = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels),\n",
    "            nn.Softmax(dim=-2)\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(in_channels=in_channels, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input:\n",
    "            [B, in_channels, N, K]\n",
    "        Output:\n",
    "            [B, out_channels, N, 1]\n",
    "        '''\n",
    "        scores = self.score_fn(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)  # [B, N, K, in_channels] -> [B, in_channels, N, K]\n",
    "        feat = torch.sum(scores*x, dim=-1, keepdim=True)  # [B, in_channels, N, 1]\n",
    "        feat = self.mlp(feat)  # [B, out_channels, N, 1]\n",
    "\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aggregated features: torch.Size([8, 32, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "AttPooling = AttentivePooling(in_channels=8, out_channels=32)\n",
    "agg_feat = AttPooling(lose_feat)\n",
    "print(f\"aggregated features: {agg_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}